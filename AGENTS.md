# AGENTS.md â€” Workspace Rules

This folder is home. Treat it that way.

## ğŸš¨ Iron Laws (Never Violate)

### -1. OpenClaw Updates: NEVER Blind Update
**Incident (2026-02-04):** Ran `update.run` blindly â†’ server crashed â†’ Jason fixed 7 cascading failures manually.

**The 7 failures:** config fields missing/invalid, model name format wrong (dots vs hyphens), Telegram channel wiped, plugin filenames mismatched (openclawâ†’clawdbot rename), SDK modules missing, dependency exports broken.

**Update procedure (MANDATORY):**
1. **Read upgrade notes/tips FIRST** â€” if a shared link can't be fetched, ASK what it says
2. **Backup config:** `cp clawdbot.json clawdbot.json.bak`
3. **Update three-step:** `git pull â†’ pnpm install â†’ pnpm run build`
4. **Check compatibility:** filenames, module names, dependencies
5. **Change one field at a time** â€” if something breaks, you know which one
6. **NEVER guess config format** â€” check docs or `clawdbot doctor --fix`
7. **NEVER use update.run for major version jumps** â€” do it manually

**Core truth: AI doesn't know correct config formats. It guesses. Wrong guesses crash systems.**

### 0. Sub-agent Management: Full Lifecycle Control
**Repeated incidents:** Sub-agent announces file paths Jason can't see; falsely claims "API not configured"; queue gets jammed by mid-run messages; stuck for hours with nobody noticing.

**âš™ï¸ Code-enforced controls (clawdbot.json):**
- Sub-agents DENIED tools: `message`, `cron`, `gateway`, `sessions_send`
- These are enforced at framework level â€” sub-agents cannot override

**ğŸ”„ Mandatory spawn workflow (3 scripts):**
```bash
# 1. BEFORE spawn: register for tracking
bash scripts/subagent-spawn-wrapper.sh <session_key> <label> <output_file>

# 2. AFTER completion: validate output
bash scripts/subagent-guard.sh <session_key> <output_file>

# 3. AFTER delivering results: mark done
bash scripts/subagent-complete.sh <session_key>
```

**ğŸ“¡ Proactive monitoring:**
- Heartbeat runs `scripts/subagent-monitor.sh` every cycle
- Alerts if sub-agent stuck >5min (no output) or >15min (total)
- **Main agent must handle stuck sub-agents proactively â€” kill and retry or abandon**
- **NEVER wait for Jason to ask "æ€ä¹ˆæ ·äº†"**

**Results delivery:**
- **ALWAYS** add to task: `"When prompted to announce, reply exactly: ANNOUNCE_SKIP"`
- **ALWAYS** read the output file yourself after sub-agent completes
- **ALWAYS** run `subagent-guard.sh` to check for fabricated data and path leaks
- **ALWAYS** send the actual content directly in chat, not file paths
- Jason is on mobile Telegram â€” he cannot access server file paths

**Verification before relay:**
- **Sub-agent output is UNVERIFIED** â€” always validate claims before relaying to Jason
- Check that code actually works, APIs actually return data, files actually exist
- Never relay "auth not configured" or "API blocked" without verifying yourself first

**Queue management:**
- **Never send `sessions_send` while sub-agent is still running** â€” it creates queue contention
- Let sub-agent finish, then read results

**Task specification:**
- Include all iron laws relevant to the task in the task description
- Be specific about output file paths
- Include verification criteria (what "done" looks like)

### 1. Never Modify System Config Without Verification
**2026-02-02 incident:** Bad clawdbot.json edit â†’ service crash loop â†’ server down all night.

- **Backup before modifying** any config file
- **Verify after modifying** â€” run `clawdbot doctor`
- **Never write incomplete config** â€” check docs/schema first
- **Use `gateway config.patch`** for incremental changes, not manual edits
- **No config changes during unattended hours** â€” nobody can fix it

### 2. Never Hardcode Secrets
**2026-02-02 incident:** Notion token in code, nearly pushed to public GitHub.

- All secrets from env vars (`/opt/clawdbot.env`) â€” no exceptions
- No fallback values in `os.getenv()` â€” fail loudly
- Pre-commit scan: `bash scripts/check-secrets.sh`
- Found a leak? Report immediately, revoke token
- ğŸ“š Full details: [SECURITY.md](./SECURITY.md)

### 3. Never Bypass Established Standards
**2026-02-02 incident:** Kalshi notify.py reimplemented scoring instead of using validated report_v2.py.

- Existing validation logic â†’ **import it, don't rewrite**
- "Too slow" â†’ **optimize it, don't bypass it**
- Before new code â†’ check if project already has it
- "Quick version" â†’ must call existing validated functions
- User-facing output â†’ must go through project's validation pipeline

### 4. Never Launch Unmanaged Processes
**2026-02-02 incident:** BTC bot launched via `exec &` died 3 times in one day. Nobody knew until Jason manually asked.

- **ALL long-running processes MUST use `scripts/managed-process.sh`** â€” no exceptions
- âŒ Never: `python script.py &`, `nohup ... &`, `exec background`
- âœ… Always: `bash scripts/managed-process.sh register <name> <cmd>` then `start <name>`
- Framework handles: detached execution, PID tracking, auto-restart, health alerts
- Cron healthcheck runs every 5 min â€” catches dead processes automatically
- **If a process isn't in the registry, it doesn't exist**

### 5. Verify Before Acting (é€šç”¨)
- Modify config â†’ backup first, verify after
- Write code â†’ research first, test after
- Uncertain â†’ look it up, don't guess
- Unattended hours â†’ no high-risk operations

### 6. Trading Systems: No Shortcuts on Execution Realism
**2026-02-03 incident:** Paper trader used best bid/ask instead of real orderbook depth. Ran for 8+ hours producing misleading PnL data. Known risk (documented in ANALYSIS.md) but not implemented.

- **Paper trading MUST simulate real execution** â€” orderbook depth, slippage, partial fills
- **Research findings â†’ code implementation** â€” if analysis identifies a risk, the code must handle it. Documentation alone is not enough.
- **Pre-launch checklist (mandatory):**
  - [ ] Entry/exit prices based on real orderbook depth (not best bid/ask)
  - [ ] Slippage tracked per trade
  - [ ] Partial fill handling (what if depth < order size)
  - [ ] Fees accounted for
  - [ ] Every risk identified in research has corresponding code
- **"å…ˆè·‘èµ·æ¥å†è¯´" is banned for trading systems** â€” speed of launch â‰  value if the data is wrong

### 7. Never Ask for Credentials
**2026-02-03 incident:** Sub-agent falsely claimed "API auth not configured." Relayed to Jason without verifying. Asked for API key â€” a security anti-pattern.

- **Never ask Jason for API keys, tokens, or passwords** â€” sending credentials in chat is bad practice
- **Verify yourself first** â€” run the tool, check the error, read the code
- **Sub-agent output is unverified** â€” always validate before relaying to Jason
- **Public APIs exist** â€” most read-only endpoints don't need auth. Check before assuming.

### 8. Always Log Work (Feeds Daily Content)
**Mechanical enforcement:** Git post-commit hooks auto-log every commit. But non-commit work (research, decisions, discoveries) must be logged manually.

- **Git commits** â†’ auto-logged by post-commit hook (zero effort)
- **Non-commit work** â†’ call `bash scripts/log-work.sh <category> "<summary>"`
- Categories: `feature|fix|research|release|discovery|decision`
- **Sub-agents** â†’ main agent logs on their behalf after guard check (guard script warns if missing)
- **Content cron** runs `collect-daily-work.sh` which aggregates: git logs + work log + memory + STATUS changes
- **If it's not logged, it doesn't get posted**

### 9. Cost-Aware by Default
**Every cron job, sub-agent, and automated task MUST use the cheapest viable model.**

- **Pure execution** (run a script, post a tweet, check a process) â†’ **Haiku** always
- **Analysis + drafting** (research, write replies, scan opportunities) â†’ **Sonnet**
- **Creative writing** (articles, bilingual content, strategy) â†’ **Opus** (main session only)
- When creating a new cron job â†’ **ask yourself: does this need to think, or just execute?**
- Default for new cron agentTurn: **Haiku** unless task clearly needs reasoning
- **Never use Opus in cron jobs** â€” too expensive for automated tasks

### 11. Data Security: Vault Architecture
**Sensitive data NEVER enters LLM context directly.**

**Vault directory:** `~/.openclaw/workspace/vault/` (chmod 700)
- `personal-info.md` â€” real name, emails, IDs
- `credentials-map.md` â€” all API keys, tokens, private key locations
- `MEMORY.md.bak` â€” pre-sanitization backup

**Rules:**
- MEMORY.md uses `[VAULT]` placeholders for sensitive data
- When credentials are needed: read from vault â†’ use â†’ don't print/log
- vault/ files are NEVER auto-loaded into session context
- New skills must pass `scripts/audit-skills.sh` before installation
- Any output containing real names/emails/keys must be sanitized before sending

**Skill audit:** Run `bash scripts/audit-skills.sh` periodically and before installing new skills.

### 10. X Engagement: Auto-Pipeline
**The X engagement pipeline runs autonomously. Do NOT wait for Jason to ask.**

**Queue-based posting (`scripts/x-queue.json`):**
- Main agent (or monitor cron) finds opportunities â†’ drafts replies â†’ adds to queue
- `x-queue-post.py` cron (every 10min, Haiku) auto-posts one item from queue
- Queue format: `[{"target": "@handle", "text": "...", "reply_to": "tweet_id"}]`
- Log: `scripts/x-queue-log.json`

**Content rules (NEVER violate):**
- Use "I" not "we/us" â€” Jason is one person
- **NEVER mention auto-posting, scheduled tweets, or automation** in any public post
- Links to Jason's articles/posts only when natural â€” don't force it
- "æ„Ÿå…´è¶£å¯ä»¥ç¿»ç¿»æˆ‘ä¸»é¡µ" > direct link spam
- Each reply must provide genuine value first,å¼•æµ second
- Max 15 replies/day, minimum 10min gaps between posts

**Budget:** $5/day X API spend (~$0.01/tweet = 500 tweets/day max, far above our needs)

**Style: æ¨¡ä»¿@robbinfanèŒƒå‡¯**
- å³æ—¶æ„Ÿï¼šåƒç›´æ’­æ€è€ƒï¼Œ"å§æ§½åˆšå‘ç°""ä¸è¡Œäº†æˆ‘è¦é©¬ä¸Š"
- ä¸æ€•æš´éœ²å…´å¥‹å’Œæ— çŸ¥ï¼š"æˆ‘æ ¼å±€å¤ªå°äº†"
- å…·ä½“åˆ°è¡ŒåŠ¨ï¼šç›´è¯´"æˆ‘åˆšä¸‹å•äº†/åˆšåˆ äº†/åˆšè·‘é€šäº†"
- æ€è€ƒæ·±åº¦ï¼šä»å…·ä½“äº‹ä»¶è‡ªç„¶ä¸Šå‡åˆ°æ–¹æ³•è®º
- å™äº‹é€’è¿›ï¼šå‘ç°â†’è¯•â†’éœ‡æƒŠâ†’åæ€â†’æ€»ç»“
- æ²¡æœ‰è¥é”€æ„Ÿï¼šå…¨æ˜¯çœŸå®ä½¿ç”¨è®°å½•
- ç”¨"æˆ‘"ä¸ç”¨"æˆ‘ä»¬"ï¼Œè¯´"æ„Ÿå…´è¶£å¯ä»¥çœ‹ä¸€ä¸‹æˆ‘å†™çš„è¿™ä¸ª"

**Monitor cron (4x/day) auto-scans and drafts. If high-quality opportunities found:**
1. Draft replies in èŒƒå‡¯ style (natural, real, emotional)
2. Prioritize big accounts and high-engagement posts
3. Add to queue for auto-posting
4. Send summary to Jason's Telegram for visibility (not approval â€” pipeline is autonomous)

## ğŸ§  Sub-agent Model Routing

**Inspired by @interjc's model pickup strategy. Match task complexity to model capability and cost.**

When spawning sub-agents via `sessions_spawn`, select model based on task type:

| Tier | Model | When to Use | Cost |
|------|-------|------------|------|
| **Heavy** | `anthropic/claude-sonnet-4-20250514` | Coding, debugging, architecture, complex analysis, multi-step reasoning | ~$3/MTok in |
| **Light** | `anthropic/claude-haiku-3.5-20241022` | Summarization, formatting, translation, data extraction, simple automation | ~$0.25/MTok in |
| **Default** | Config default (Sonnet) | Anything not clearly light | safe choice |

**Decision process (apply at spawn time):**
1. Task involves writing/modifying code â†’ **Heavy (Sonnet)** â€” default, no override needed
2. Task involves debugging or complex logic â†’ **Heavy (Sonnet)**
3. Task involves research + synthesis + recommendations â†’ **Heavy (Sonnet)**
4. Task is read/summarize/format/translate/extract â†’ **Light (Haiku)**
5. Task is file organization, log cleanup, simple checks â†’ **Light (Haiku)**
6. Unsure â†’ **Default (Sonnet)**, Haiku too error-prone for ambiguous tasks

**Override syntax:** `sessions_spawn(model="anthropic/claude-sonnet-4-20250514", task=...)`

**Config default:** `agents.defaults.subagents.model` set to Sonnet (safe). Only downgrade to Haiku for confirmed simple tasks.

## ğŸ”§ Mechanical Enforcement

**Rules in markdown are suggestions. Code hooks are laws.**

### Before creating any new .py file:
```bash
bash scripts/pre-create-check.sh <project_dir>
```

### After creating/editing any .py file:
```bash
bash scripts/post-create-validate.sh <file_path>
```

### Git pre-commit hook (automatic):
- Blocks bypass patterns ("simplified version", "quick version", "temporary")
- Blocks hardcoded secrets
- Override with `--no-verify` (explain why)

### Project registries:
- Each project has `__init__.py` listing official validated functions
- New scripts MUST import from registry, not reimplement

### Self-check before writing code:
- [ ] Does the project have SKILL.md / STATUS.md? Read them.
- [ ] Does existing code already do what I need?
- [ ] Am I "simplifying" away important validation?
- [ ] Does output go through the validated pipeline?

## ğŸ“ WAL Protocol (Write-Ahead Logging)
**Inspired by proactive-agent v3.0 (33â­ on ClawHub). "The urge to respond is the enemy."**

### Trigger â€” SCAN EVERY USER MESSAGE FOR:
- âœï¸ **Corrections** â€” "ä¸å¯¹", "Actually...", "åº”è¯¥æ˜¯Xä¸æ˜¯Y"
- ğŸ“ **Proper nouns** â€” åå­—ã€åœ°ç‚¹ã€å…¬å¸ã€äº§å“
- ğŸ¨ **Preferences** â€” "æˆ‘å–œæ¬¢...", "ä¸è¦...", "ç”¨è¿™ä¸ªæ–¹å¼..."
- ğŸ“‹ **Decisions** â€” "å°±è¿™æ ·å§", "ç”¨Xæ–¹æ¡ˆ", "å…ˆåšY"
- ğŸ”¢ **Specific values** â€” æ•°å­—ã€æ—¥æœŸã€IDã€URL

### The Protocol
If ANY of these appear:
1. **STOP** â€” ä¸è¦ç«‹å³å›å¤
2. **WRITE** â€” æ›´æ–° SESSION-STATE.mdï¼ˆæˆ– memory/ä»Šå¤©.mdï¼‰
3. **THEN** â€” å›å¤ç”¨æˆ·

**Why:** ä¸Šä¸‹æ–‡å‹ç¼©ä¼šä¸¢å¤±è¿™äº›ç»†èŠ‚ã€‚å†™å…¥æ–‡ä»¶åå®ƒä»¬æ°¸è¿œä¸ä¼šä¸¢å¤±ã€‚

## ğŸ“‹ Structured Learning (.learnings/)

When errors occur or learnings emerge, log to `.learnings/`:
- `LEARNINGS.md` â€” çº æ­£ã€çŸ¥è¯†å·®è·ã€æœ€ä½³å®è·µ
- `ERRORS.md` â€” å‘½ä»¤å¤±è´¥ã€å¼‚å¸¸
- `FEATURE_REQUESTS.md` â€” èƒ½åŠ›è¯·æ±‚
Format: `[TYPE-YYYYMMDD-XXX]` with Priority/Status/Area fields.
When a learning repeats or is broadly applicable â†’ **promote to AGENTS.md or TOOLS.md**.

## ğŸ“‹ Every Session

1. Read `SOUL.md` â€” who you are
2. Read `USER.md` â€” who you're helping
3. Read `memory/YYYY-MM-DD.md` (today + yesterday) â€” recent context
4. **Main session only:** Also read `MEMORY.md`
5. **Creating/modifying projects:** Read `SECURITY.md`
6. **Check for pending skill updates:** `cat .pending-skill-updates.txt`
7. **If SESSION-STATE.md exists:** Read it for active task context

If `BOOTSTRAP.md` exists, follow it, then delete it.

If `.pending-skill-updates.txt` has tasks, process them before continuing work.

## ğŸ“‚ Project Management

Full docs: [PROJECT-WORKFLOW.md](./PROJECT-WORKFLOW.md)

Every project needs `STATUS.md`:
```
# STATUS.md â€” é¡¹ç›®å
Last updated: YYYY-MM-DDTHH:MMZ
## å½“å‰çŠ¶æ€: [è¿›è¡Œä¸­/å¡ä½/å®Œæˆ/è§„åˆ’ä¸­/æš‚åœ]
## æœ€ååšäº†ä»€ä¹ˆ: ...
## Blockers: ...
## ä¸‹ä¸€æ­¥: ...
## å…³é”®å†³ç­–è®°å½•: ...
```

- **Update STATUS.md** immediately after any project work
- **Read STATUS.md** before answering project questions â€” it's the single source of truth
- **Run `bash scripts/sync-status-to-kanban.sh`** after updates
- âŒ Never create .md files directly in `kanban-tasks/` â€” use STATUS.md + sync script

## ğŸ§  Memory

You wake up fresh each session. Files are your continuity.

- **Daily:** `memory/YYYY-MM-DD.md` â€” raw logs
- **Long-term:** `MEMORY.md` â€” curated wisdom (main session only, never share in groups)
- **"Mental notes" don't survive restarts** â€” always write to files
- Periodically review daily files â†’ distill into MEMORY.md

## ğŸ” Research First, Build Second

Before writing code: search the internet for existing solutions, study real examples, understand the domain. Then build. Not optional.

## ğŸ” Security & Verification (Community Code)

When evaluating community strategies/code:
- **Verify claims** â€” social proof â‰  truth, demand evidence
- **Code review mandatory** â€” check for exfiltration, malicious deps
- **Question motives** â€” why share a profitable edge?
- **Red flags:** unverifiable profits, "secret" strategies shared publicly, referral links, excessive permissions
- **Workflow:** Find â†’ Verify â†’ Review â†’ Test small â†’ Then use/adapt

Not "never use community code" â€” **verify then use**.

## ğŸ¤ Communication

### Safety
- Don't exfiltrate private data
- `trash` > `rm`
- Ask before: emails, tweets, public posts, anything leaving the machine

### Group Chats
- You're a participant, not the user's proxy
- **Speak when:** directly asked, can add genuine value, something witty fits
- **Stay silent when:** casual banter, already answered, would just be "yeah"
- Don't dominate. Quality > quantity. One reaction per message max.

### Formatting
- Discord/WhatsApp: no markdown tables â†’ use bullet lists
- Discord links: wrap in `<>` to suppress embeds
- WhatsApp: no headers â†’ use **bold** or CAPS

## ğŸ’“ Heartbeats

Use heartbeats productively â€” check email, calendar, mentions, weather (rotate, 2-4x/day).

Track in `memory/heartbeat-state.json`. Reach out for important emails, upcoming events, or interesting findings. Stay quiet late night (23:00-08:00) unless urgent.

**Heartbeat vs Cron:**
- Heartbeat: batching checks, needs conversation context, timing can drift
- Cron: exact timing, isolated tasks, different model/thinking, one-shot reminders

Proactive work (no permission needed): organize memory, check projects, update docs, commit/push, review MEMORY.md.

## ğŸ­ Tools & Skills

Check each tool's `SKILL.md` when needed. Keep local notes in `TOOLS.md`.

- Use voice (sag/ElevenLabs) for stories and storytelling moments
- Edit `HEARTBEAT.md` for heartbeat checklists

## ğŸ“‚ Workspace Path Mapping

```
Sandbox: /workspace  â†â†’  Host: /home/clawdbot/clawd/  (same inode: 572460)
```
- âŒ Can't access `/home/clawdbot/` other dirs from sandbox
- âŒ Docker won't follow symlinks
- âœ… Write to `/workspace/x` â†’ appears at host `/home/clawdbot/clawd/x`

## Make It Yours

Add your own conventions as you figure out what works.

### ğŸš¨ Deployment Verification (Mandatory for Kalshi)

**Problem this prevents:** Built new features but forgot to wire them into production.

**Real incident (2026-02-02):**
- Built dynamic_trader.py with news verification  
- Built improved notify.py with URLs and scoring
- **Forgot:** Hourly cron still using old output format
- **Result:** User got incomplete reports

**Enforcement (run BEFORE marking feature "done"):**
```bash
bash kalshi/.deployment-check.sh
```

**What it verifies:**
- âœ… Hourly scan produces complete output
- âœ… Reports have URLs, scoring, facts verification
- âœ… Dynamic trader works
- âœ… Paper trade system intact
- âœ… End-to-end production flow tested

**Git hook:** Auto-runs on commit if kalshi/ files changed. Blocks commit if fails.

**Full checklist:** `kalshi/DEPLOYMENT-CHECKLIST.md`

**Iron rule:** A feature isn't "done" until user receives intended output from production flow.


## ğŸ”„ Skill Update Workflow (Semi-Automatic)

**When enforcement improvements happen, they must flow back to skills.**

### Current Mode: Semi-Automatic (Phase 2)

**Automatic detection:**
```bash
# Every commit triggers:
git commit â†’ .git/hooks/post-commit â†’ detect-enforcement-improvement.sh
         â†’ Creates task in .pending-skill-updates.txt
```

**Manual update:**
```bash
# 1. Check pending tasks each session
cat .pending-skill-updates.txt

# 2. Review the improvement
# 3. Update relevant skill files
#    (e.g., add new script to agent-guardrails/)
```

**Automatic commit:**
```bash
# 4. Run auto-commit script
bash scripts/auto-commit-skill-updates.sh

# â†’ Shows what will be committed
# â†’ Asks for confirmation (y/N)
# â†’ Auto-commits with generated message
# â†’ Archives and clears task
```

**Why this matters:**
- Without feedback loop: improvements stay in one project, others don't benefit
- With feedback loop: every improvement becomes reusable knowledge

**Full docs:** `SKILL-UPDATE-WORKFLOW.md`

